{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3604f6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ee1e6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The dog'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()[0]+\" \"+sentence.split()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a4970a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'quick' in sentence  #check whether the word 'quick' belongs to that text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cb9a526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The brown is present in the sentence\n"
     ]
    }
   ],
   "source": [
    "if \"brown\" in sentence:\n",
    "    print('The brown is present in the sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b9546f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.index('fox')     #find out the index value of the word 'fox'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29733b4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nworb'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()[2][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0eda8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thedog\n"
     ]
    }
   ],
   "source": [
    "words=sentence.split()\n",
    "first_word=words[0]\n",
    "last_word=words[len(words)-1]\n",
    "concat_word=first_word+last_word\n",
    "print(concat_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb0e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence ='The quick brown fox jumps over the lazy dog'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c57fc0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'brown', 'jumps', 'the', 'dog']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in range(len(words)) if i%2==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7451a706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'brown', 'jumps', 'the', 'dog']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[words[i] for i in range(len(words)) if i%2==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6ee55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\abdul\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba08a48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e65e1ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8ad56f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'learning', 'python', 'for', 'NLP']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"i am learning python for NLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d37882",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='''\n",
    "Google Colaboratory\n",
    "Colab is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, \n",
    "including GPUs and TPUs. Colab is especially well suited to machine learning, data science, and education.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baa52658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bdbfb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nGoogle Colaboratory\\nColab is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, \\nincluding GPUs and TPUs.',\n",
       " 'Colab is especially well suited to machine learning, data science, and education.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95a93f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba7e4483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'Colaboratory', 'Colab', 'is', 'a', 'hosted', 'Jupyter', 'Notebook', 'service', 'that', 'requires', 'no', 'setup', 'to', 'use', 'and', 'provides', 'free', 'access', 'to', 'computing', 'resources', ',', 'including', 'GPUs', 'and', 'TPUs', '.']\n",
      "['Colab', 'is', 'especially', 'well', 'suited', 'to', 'machine', 'learning', ',', 'data', 'science', ',', 'and', 'education', '.']\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sent_tokenize(paragraph))):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f17da9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'reading', 'NLP', 'Fundamnetals']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('reading', 'VBG'),\n",
       " ('NLP', 'NNP'),\n",
       " ('Fundamnetals', 'NNS')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "words=word_tokenize('I am reading NLP  Fundamnetals')\n",
    "print(words)\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cb1e04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Google', 'Colaboratory', 'Colab', 'is', 'a', 'hosted', 'Jupyter', 'Notebook', 'service', 'that', 'requires', 'no', 'setup', 'to', 'use', 'and', 'provides', 'free', 'access', 'to', 'computing', 'resources', ',', 'including', 'GPUs', 'and', 'TPUs', '.']\n",
      "[('Google', 'NNP'), ('Colaboratory', 'NNP'), ('Colab', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('hosted', 'JJ'), ('Jupyter', 'NNP'), ('Notebook', 'NNP'), ('service', 'NN'), ('that', 'WDT'), ('requires', 'VBZ'), ('no', 'DT'), ('setup', 'NN'), ('to', 'TO'), ('use', 'VB'), ('and', 'CC'), ('provides', 'VBZ'), ('free', 'JJ'), ('access', 'NN'), ('to', 'TO'), ('computing', 'VBG'), ('resources', 'NNS'), (',', ','), ('including', 'VBG'), ('GPUs', 'NNP'), ('and', 'CC'), ('TPUs', 'NNP'), ('.', '.')]\n",
      "['Colab', 'is', 'especially', 'well', 'suited', 'to', 'machine', 'learning', ',', 'data', 'science', ',', 'and', 'education', '.']\n",
      "[('Colab', 'NNP'), ('is', 'VBZ'), ('especially', 'RB'), ('well', 'RB'), ('suited', 'VBN'), ('to', 'TO'), ('machine', 'NN'), ('learning', 'NN'), (',', ','), ('data', 'NNS'), ('science', 'NN'), (',', ','), ('and', 'CC'), ('education', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sent_tokenize(paragraph))):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    print(words)\n",
    "    print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87e63241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89a43622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "stop_words=stopwords.words('English')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "61df2554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'Python.It', 'is', 'one', 'of', 'the', 'most', 'popular', 'program']\n"
     ]
    }
   ],
   "source": [
    "sentence = 'I am learning Python.It is one of the most popular program'\n",
    "sentence_words=word_tokenize(sentence)\n",
    "print(sentence_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8934020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I learning Python.It one popular program\n"
     ]
    }
   ],
   "source": [
    "sentence_no_stops=' '.join([word for  word in sentence_words if word not in stop_words])\n",
    "print(sentence_no_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ea82199",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1672310733.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [26]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Text normalisation\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Text normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29cfd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence='I visited US from UK on 22-10-18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c49a7159",
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_sentence=sentence.replace('US','United States').replace('UK','United kingdom').replace('18','2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a00f172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United States from United kingdom on 22-10-2018\n"
     ]
    }
   ],
   "source": [
    "print(replace_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "955b3657",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autocorrect in c:\\users\\abdul\\anaconda3\\lib\\site-packages (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd261526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from autocorrect import Speller\n",
    "spell=Speller()\n",
    "spell('Natureal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f49583d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natual', 'Languae', 'Processin', 'deals', 'with', 'the', 'insights', 'with', 'art', 'of', ';']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " 'deals',\n",
       " 'with',\n",
       " 'the',\n",
       " 'insights',\n",
       " 'with',\n",
       " 'art',\n",
       " 'of',\n",
       " ';']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=word_tokenize('Natual Languae Processin deals with the insights with art of ;')\n",
    "print(sentence)\n",
    "from autocorrect import Speller\n",
    "spell=Speller()\n",
    "[spell(word) for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d72d0390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing deals with the insights with art of ;\n"
     ]
    }
   ],
   "source": [
    "sentence_corrected=' '.join ([spell(word) for word in sentence])\n",
    "print(sentence_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c8d26f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "stemmer=nltk.stem.PorterStemmer()\n",
    "stemmer.stem('Production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f18a5cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer=PorterStemmer()\n",
    "stemmer.stem('production')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "262c086f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fairli\n",
      "fri\n",
      "fire\n"
     ]
    }
   ],
   "source": [
    "print(stemmer.stem(\"fairly\"))\n",
    "print(stemmer.stem('fries'))\n",
    "print(stemmer.stem('Firing'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9147336b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hari'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('hary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64be2dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization\n",
    "import nltk\n",
    "nltk.download('omw-1.4')  \n",
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8640264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8c9a3ec9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'history'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('history')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a02d2e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stemming'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('stemming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16719267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'amazing'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer =WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('amazing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8e3b3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#named entity recognition(NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e3fd597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "sentence='we are reading a book published by packet which is based out of Birmingham'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "76ae1b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tree('NE', [('Birmingham', 'NNP')])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=nltk.ne_chunk(nltk.pos_tag(word_tokenize(sentence)),binary=True)\n",
    "[a for a in i if len(a)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "162afdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('bank.v.07')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "sentence1=\"keep ypur savings in the bank\"\n",
    "sentence2=\"it is so risky to drive over the banks  of the river\"\n",
    "print(lesk(word_tokenize(sentence1), 'bank'))\n",
    "print(lesk(word_tokenize(sentence2), 'bank'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "784e75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph='''\n",
    "Google Colaboratory\n",
    "Colab is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, \n",
    "including GPUs and TPUs. Colab is especially well suited to machine learning, data science, and education.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "22baa3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the texts\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "corpus=[]\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('sub[^a=zA=Z]','',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word)for word in review if not word in set(stopwords.words('english'))]\n",
    "    review= ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(max_features=1500)\n",
    "x=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "74d27604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['googl colaboratori colab host jupyt notebook servic requir setup use provid free access comput resources, includ gpu tpus.',\n",
       " 'colab especi well suit machin learning, data science, education.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d8d8feb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'googl': 8,\n",
       " 'colaboratori': 2,\n",
       " 'colab': 1,\n",
       " 'host': 10,\n",
       " 'jupyt': 12,\n",
       " 'notebook': 15,\n",
       " 'servic': 20,\n",
       " 'requir': 17,\n",
       " 'setup': 21,\n",
       " 'use': 24,\n",
       " 'provid': 16,\n",
       " 'free': 7,\n",
       " 'access': 0,\n",
       " 'comput': 3,\n",
       " 'resources': 18,\n",
       " 'includ': 11,\n",
       " 'gpu': 9,\n",
       " 'tpus': 23,\n",
       " 'especi': 6,\n",
       " 'well': 25,\n",
       " 'suit': 22,\n",
       " 'machin': 14,\n",
       " 'learning': 13,\n",
       " 'data': 4,\n",
       " 'science': 19,\n",
       " 'education': 5}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87085e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   ...  16  17  18  19  20  21  22  \\\n",
       "0   1   1   1   1   0   0   0   1   1   1  ...   1   1   1   0   1   1   0   \n",
       "1   0   1   0   0   1   1   1   0   0   0  ...   0   0   0   1   0   0   1   \n",
       "\n",
       "   23  24  25  \n",
       "0   1   1   0  \n",
       "1   0   0   1  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c7703638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method CountVectorizer.get_feature_names of CountVectorizer(max_features=1500)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "636df0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv=TfidfVectorizer(max_features=1500)\n",
    "x=cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6a162211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.23900309, 0.17005267, 0.23900309, 0.23900309, 0.        ,\n",
       "        0.        , 0.        , 0.23900309, 0.23900309, 0.23900309,\n",
       "        0.23900309, 0.23900309, 0.23900309, 0.        , 0.        ,\n",
       "        0.23900309, 0.23900309, 0.23900309, 0.23900309, 0.        ,\n",
       "        0.23900309, 0.23900309, 0.        , 0.23900309, 0.23900309,\n",
       "        0.        ],\n",
       "       [0.        , 0.24395573, 0.        , 0.        , 0.34287126,\n",
       "        0.34287126, 0.34287126, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.34287126, 0.34287126,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.34287126,\n",
       "        0.        , 0.        , 0.34287126, 0.        , 0.        ,\n",
       "        0.34287126]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b0083a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 26)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ae21dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "852ad9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==3.7.4 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy==3.7.4) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy==3.7.4) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==3.7.4) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy==3.7.4) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==3.7.4) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy==3.7.4) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy==3.7.4) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from jinja2->spacy==3.7.4) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy==3.7.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c345d015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-md==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.7.1/en_core_web_md-3.7.1-py3-none-any.whl (42.8 MB)\n",
      "     ---------------------------------------- 42.8/42.8 MB 3.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.8.0,>=3.7.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from en-core-web-md==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.64.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2024.2.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\abdul\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-md==3.7.1) (2.0.1)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c3521072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"Elon Reeve Musk is a businessman and investor.\n",
    "     He is the founder, chairman, CEO, and CTO of SpaceX, angel investor, CEO, product architect, and former chairman of Tesla,\n",
    "     Inc. owner, executive chairman, and CTO of X Corp. founder of the Boring Company and xAI co-founder of Neuralink and OpenAI, and president of the Musk Foundation. \n",
    "     He is the second wealthiest person in the world, with an estimated net worth of US$232 billion as of December 2023, according to the Bloomberg Billionaires Index, \n",
    "     and $182.6 billion according to Forbes, primarily from his ownership stakes in Tesla and SpaceX.\"\"\"\n",
    "\n",
    "# Process the text with SpaCy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1990437f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Tokens (without stopwords): ['Elon', 'Reeve', 'Musk', 'businessman', 'investor', '.', '\\n     ', 'founder', ',', 'chairman', ',', 'CEO', ',', 'CTO', 'SpaceX', ',', 'angel', 'investor', ',', 'CEO', ',', 'product', 'architect', ',', 'chairman', 'Tesla', ',', '\\n     ', 'Inc.', 'owner', ',', 'executive', 'chairman', ',', 'CTO', 'X', 'Corp.', 'founder', 'Boring', 'Company', 'xAI', 'co', '-', 'founder', 'Neuralink', 'OpenAI', ',', 'president', 'Musk', 'Foundation', '.', '\\n     ', 'second', 'wealthiest', 'person', 'world', ',', 'estimated', 'net', 'worth', 'US$', '232', 'billion', 'December', '2023', ',', 'according', 'Bloomberg', 'Billionaires', 'Index', ',', '\\n     ', '$', '182.6', 'billion', 'according', 'Forbes', ',', 'primarily', 'ownership', 'stakes', 'Tesla', 'SpaceX.']\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens=[token.text for token in doc if not token.is_stop]\n",
    "print('Filtered Tokens (without stopwords):',filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff4e35c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmas (without stopwords): ['Elon', 'Reeve', 'Musk', 'businessman', 'investor', '.', '\\n     ', 'founder', ',', 'chairman', ',', 'ceo', ',', 'CTO', 'SpaceX', ',', 'angel', 'investor', ',', 'ceo', ',', 'product', 'architect', ',', 'chairman', 'Tesla', ',', '\\n     ', 'Inc.', 'owner', ',', 'executive', 'chairman', ',', 'CTO', 'X', 'Corp.', 'founder', 'Boring', 'Company', 'xAI', 'co', '-', 'founder', 'Neuralink', 'OpenAI', ',', 'president', 'Musk', 'Foundation', '.', '\\n     ', 'second', 'wealthy', 'person', 'world', ',', 'estimate', 'net', 'worth', 'us$', '232', 'billion', 'December', '2023', ',', 'accord', 'Bloomberg', 'Billionaires', 'Index', ',', '\\n     ', '$', '182.6', 'billion', 'accord', 'Forbes', ',', 'primarily', 'ownership', 'stake', 'Tesla', 'SpaceX.']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "lemmas = [token.lemma_ for token in doc if not token.is_stop]\n",
    "print(\"Lemmas (without stopwords):\", lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f60a1063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part-of-speech tags (without stopwords): [('Elon', 'PROPN'), ('Reeve', 'PROPN'), ('Musk', 'PROPN'), ('businessman', 'NOUN'), ('investor', 'NOUN'), ('.', 'PUNCT'), ('\\n     ', 'SPACE'), ('founder', 'NOUN'), (',', 'PUNCT'), ('chairman', 'NOUN'), (',', 'PUNCT'), ('CEO', 'NOUN'), (',', 'PUNCT'), ('CTO', 'PROPN'), ('SpaceX', 'PROPN'), (',', 'PUNCT'), ('angel', 'NOUN'), ('investor', 'NOUN'), (',', 'PUNCT'), ('CEO', 'NOUN'), (',', 'PUNCT'), ('product', 'NOUN'), ('architect', 'NOUN'), (',', 'PUNCT'), ('chairman', 'NOUN'), ('Tesla', 'PROPN'), (',', 'PUNCT'), ('\\n     ', 'SPACE'), ('Inc.', 'PROPN'), ('owner', 'NOUN'), (',', 'PUNCT'), ('executive', 'ADJ'), ('chairman', 'NOUN'), (',', 'PUNCT'), ('CTO', 'PROPN'), ('X', 'PROPN'), ('Corp.', 'PROPN'), ('founder', 'NOUN'), ('Boring', 'PROPN'), ('Company', 'PROPN'), ('xAI', 'PROPN'), ('co', 'NOUN'), ('-', 'NOUN'), ('founder', 'NOUN'), ('Neuralink', 'PROPN'), ('OpenAI', 'PROPN'), (',', 'PUNCT'), ('president', 'NOUN'), ('Musk', 'PROPN'), ('Foundation', 'PROPN'), ('.', 'PUNCT'), ('\\n     ', 'SPACE'), ('second', 'ADV'), ('wealthiest', 'ADJ'), ('person', 'NOUN'), ('world', 'NOUN'), (',', 'PUNCT'), ('estimated', 'VERB'), ('net', 'ADJ'), ('worth', 'NOUN'), ('US$', 'SYM'), ('232', 'NUM'), ('billion', 'NUM'), ('December', 'PROPN'), ('2023', 'NUM'), (',', 'PUNCT'), ('according', 'VERB'), ('Bloomberg', 'PROPN'), ('Billionaires', 'PROPN'), ('Index', 'PROPN'), (',', 'PUNCT'), ('\\n     ', 'SPACE'), ('$', 'SYM'), ('182.6', 'NUM'), ('billion', 'NUM'), ('according', 'VERB'), ('Forbes', 'PROPN'), (',', 'PUNCT'), ('primarily', 'ADV'), ('ownership', 'NOUN'), ('stakes', 'NOUN'), ('Tesla', 'PROPN'), ('SpaceX.', 'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "# Part-of-speech tagging\n",
    "pos_tags = [(token.text, token.pos_) for token in doc if not token.is_stop]\n",
    "print(\"Part-of-speech tags (without stopwords):\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9dad9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Reeve Musk || PERSON || People, including fictional\n",
      "SpaceX || ORG || Companies, agencies, institutions, etc.\n",
      "Tesla || ORG || Companies, agencies, institutions, etc.\n",
      "X Corp. || ORG || Companies, agencies, institutions, etc.\n",
      "the Boring Company || ORG || Companies, agencies, institutions, etc.\n",
      "xAI || PERSON || People, including fictional\n",
      "Neuralink || ORG || Companies, agencies, institutions, etc.\n",
      "OpenAI || ORG || Companies, agencies, institutions, etc.\n",
      "the Musk Foundation || ORG || Companies, agencies, institutions, etc.\n",
      "second || ORDINAL || \"first\", \"second\", etc.\n",
      "US$232 billion || MONEY || Monetary values, including unit\n",
      "December 2023 || DATE || Absolute or relative dates or periods\n",
      "the Bloomberg Billionaires Index || ORG || Companies, agencies, institutions, etc.\n",
      "$182.6 billion || MONEY || Monetary values, including unit\n",
      "Forbes || ORG || Companies, agencies, institutions, etc.\n",
      "Tesla || ORG || Companies, agencies, institutions, etc.\n",
      "SpaceX. || ORG || Companies, agencies, institutions, etc.\n"
     ]
    }
   ],
   "source": [
    "# Named Entity Recognition (NER)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,\"||\",ent.label_,\"||\",spacy.explain(ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df6bb643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Elon Reeve Musk\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " is a businessman and investor.<br>     He is the founder, chairman, CEO, and CTO of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SpaceX\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", angel investor, CEO, product architect, and former chairman of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ",<br>     Inc. owner, executive chairman, and CTO of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    X Corp.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " founder of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Boring Company\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    xAI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " co-founder of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Neuralink\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    OpenAI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", and president of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Musk Foundation\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ". <br>     He is the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    second\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " wealthiest person in the world, with an estimated net worth of \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US$232 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " as of \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    December 2023\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", according to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the Bloomberg Billionaires Index\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", <br>     and \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $182.6 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " according to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Forbes\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", primarily from his ownership stakes in \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Tesla\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SpaceX.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent',jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e8a904a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#whitespace tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cce76275",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python', 'is', 'best', '%%', 'programming', 'language']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import whitespace tokenizer() method from m=nltk\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "tk=WhitespaceTokenizer()\n",
    "text='Python \\nis\\t best %% programming language'\n",
    "t=tk.tokenize(text)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4e329abf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Python \\nis', ' best %% programming language']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TabTokenizer\n",
    "tk=TabTokenizer()\n",
    "text='Python \\nis\\t best %% programming language'\n",
    "t=tk.tokenize(text)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "62a79366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Py', '*', 'thon', '..$$&', 'is', 'used', 'for', 'Datascience', 'and', 'ML']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tk=WordPunctTokenizer()\n",
    "text='Py*thon..$$& \\nis\\t used for Datascience and ML'\n",
    "t=tk.tokenize(text)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "af4dcec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 6 conditions>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "tk=ConditionalFreqDist()\n",
    "txt='Python for Python is Best Language Python is Easy to learn'\n",
    "for word in word_tokenize(txt):\n",
    "    condition=len(word)\n",
    "    tk[condition][word]+=1\n",
    "tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0a2bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import ConditionalFreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "tk=ConditionalFreqDist()\n",
    "txt='Python for Python is best Language'\n",
    "for word in word_tokenize(txt):\n",
    "    condition=len(word)\n",
    "    tk[condition][word]+=1\n",
    "tk\n",
    "tk.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d708932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781644b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5091c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3c192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84552fa5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
